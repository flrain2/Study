{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLTiEofA73S3yK+GWUqr1n"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXXXOo4oPk7F"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import urllib.request\n",
        "import platform\n",
        "import calendar\n",
        "import requests\n",
        "import datetime\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "from bs4 import BeautifulSoup\n",
        "from multiprocessing import Process\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client_id = \"ID\"\n",
        "client_secret = \"PW\"\n",
        "\n",
        "title1 = []\n",
        "pDate1 = []\n",
        "description1 = []\n",
        "org_link1 =[]\n",
        "link1 = []\n",
        "dt_now = datetime.datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "#[CODE 1]\n",
        "def getRequestUrl(url):\n",
        "    req = urllib.request.Request(url)\n",
        "    req.add_header(\"X-Naver-Client-Id\", client_id)\n",
        "    req.add_header(\"X-Naver-Client-Secret\", client_secret)\n",
        "\n",
        "    try:\n",
        "        response = urllib.request.urlopen(req)\n",
        "        if response.getcode() == 200:\n",
        "            print(\"[%s]Url Request Success\" % datetime.datetime.now())\n",
        "            return response.read().decode('utf-8')\n",
        "    except Exception as e :\n",
        "        print(e)\n",
        "        print(\"[%s] Error for URL : %s\" % (datetime.datetime.now(), url))\n",
        "        return None\n",
        "\n",
        "#[CODE 2]\n",
        "def getNaverSearch(node, srcText, start, display):\n",
        "    base = \"https://openapi.naver.com/v1/search\"\n",
        "    node = \"/%s.json\" % node\n",
        "    parameters = \"?query=%s&start=%s&display=%s\" % (urllib.parse.quote(srcText), start, display)\n",
        "\n",
        "    url = base + node + parameters\n",
        "    responseDecode = getRequestUrl(url)     #[CODE 1]\n",
        "\n",
        "    if(responseDecode == None):\n",
        "        return None\n",
        "    else:\n",
        "        return json.loads(responseDecode)\n",
        "\n",
        "#[CODE 3]\n",
        "def getPostData(post, jsonResult, cnt):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "\n",
        "    title = post['title']\n",
        "    titlec = re.sub(cleanr, '', title)\n",
        "    description = post['description']\n",
        "    descriptionc = re.sub(cleanr, '', description)\n",
        "    org_link = post['originallink']\n",
        "    link = post['link']\n",
        "\n",
        "    pDate = datetime.datetime.strptime(post['pubDate'], '%a, %d %b %Y %H:%M:%S +0900')\n",
        "    pDate = pDate.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    title1.append(titlec)\n",
        "    description1.append(descriptionc)\n",
        "    org_link1.append(org_link)\n",
        "    link1.append(link)\n",
        "    pDate1.append(pDate)\n",
        "\n",
        "    jsonResult.append({'cnt':cnt, 'title':title, 'description':description,\n",
        "                       'org_link':org_link, 'link':org_link, 'pDate':pDate})\n",
        "    return\n",
        "\n",
        "#[CODE 0]\n",
        "def main():\n",
        "    node = 'news'  #크롤링한 대상\n",
        "    srcText = input('검색어를 입력하세요: ')\n",
        "    cnt = 0\n",
        "    jsonResult = []\n",
        "\n",
        "    jsonResponse = getNaverSearch(node, srcText, 1, 100)  #[CODE 2]\n",
        "    total = jsonResponse['total']\n",
        "\n",
        "    while((jsonResponse != None) and (jsonResponse['display'] != 0)):\n",
        "        for post in jsonResponse['items']:\n",
        "            cnt+= 1\n",
        "            getPostData(post, jsonResult, cnt)   #[CODE 3]\n",
        "\n",
        "        start = jsonResponse['start'] + jsonResponse['display']\n",
        "        jsonResponse = getNaverSearch(node, srcText, start, 100)   #[CODE 2]\n",
        "\n",
        "    print('전체 검색 : %d 건' %total)\n",
        "\n",
        "    #with open('%s_naver_%s.json' % (srcText, node), 'w', encoding='utf8') as outfile:\n",
        "    #    jsonFile = json.dumps(jsonResult, indent=4, sort_keys = True, ensure_ascii = False)\n",
        "\n",
        "    #    outfile.write(jsonFile)\n",
        "\n",
        "    df=pd.DataFrame([title1, pDate1, description1, org_link1, link1 ]).T\n",
        "    df.columns=['제목','날짜','내용','원래주소','네이버뉴스주소']\n",
        "\n",
        "    print(\"가져온 데이터 : %d 건\" %(cnt))\n",
        "    #print('%s_naver_%s.json SAVED' % (srcText, node))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "_3YeawWFRKcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "post_urls = link1"
      ],
      "metadata": {
        "id": "10Eevbj7SdAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 클렌징\n",
        "class ArticleParser(object):\n",
        "    special_symbol = re.compile('[\\{\\}\\[\\]\\/?,;:|\\)*~`!^\\-_+<>@\\#$&▲▶◆◀■【】\\\\\\=\\(\\'\\\"]')\n",
        "    content_pattern = re.compile('본문 내용 | TV플레이어 | 동영상 뉴스 | flash 오류를 우회하기 위한 함수 추가function  flash removeCallback|tt | 앵커 멘트 | xa0')\n",
        "\n",
        "    @classmethod\n",
        "    def clear_content(cls, text):\n",
        "        # 기사 본문에서 필요없는 특수문자 및 본문 양식 등을 다 지움\n",
        "        newline_symbol_removed_text = text.replace('\\\\n', '').replace('\\\\t', '').replace('\\\\r', '') #줄바꿈, 탭, 커서를 앞의행으로 이동 제거\n",
        "        special_symbol_removed_content = re.sub(cls.special_symbol, ' ', newline_symbol_removed_text) #특수문자 제거\n",
        "        end_phrase_removed_content = re.sub(cls.content_pattern, '', special_symbol_removed_content) #일정 패턴 제거\n",
        "        blank_removed_content = re.sub(' +', ' ', end_phrase_removed_content).lstrip()  # 공백 에러 삭제\n",
        "        reversed_content = ''.join(reversed(blank_removed_content))  # 기사 내용을 reverse 한다.\n",
        "        content = ''\n",
        "        for i in range(0, len(blank_removed_content)):\n",
        "            # reverse 된 기사 내용중, \".다\"로 끝나는 경우 기사 내용이 끝난 것이기 때문에 기사 내용이 끝난 후의 광고, 기자 등의 정보는 다 지움\n",
        "            if reversed_content[i:i + 2] == '.다':\n",
        "                content = ''.join(reversed(reversed_content[i:]))\n",
        "                break\n",
        "        return content\n",
        "\n",
        "    @classmethod\n",
        "    def clear_headline(cls, text):\n",
        "        # 기사 제목에서 필요없는 특수문자들을 지움\n",
        "        newline_symbol_removed_text = text.replace('\\\\n', '').replace('\\\\t', '').replace('\\\\r', '')\n",
        "        special_symbol_removed_headline = re.sub(cls.special_symbol, '', newline_symbol_removed_text)\n",
        "        return special_symbol_removed_headline"
      ],
      "metadata": {
        "id": "OMFVLKaZYuqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 오류링크 제거\n",
        "search = \"sid=106\"\n",
        "for word in post_urls:\n",
        "    if search in word:\n",
        "        post_urls.remove(word)\n",
        "\n",
        "search = \"0004316027?sid=105\"\n",
        "for word in post_urls:\n",
        "    if search in word:\n",
        "        post_urls.remove(word)\n"
      ],
      "metadata": {
        "id": "v4YXIgM1ZByf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = []\n",
        "for content_url in post_urls:\n",
        "  # 신문사 구분 link 뽑아내기\n",
        "  title = content_url.split('/')[2]\n",
        "  print(content_url)\n",
        "  # 크롤링 대기 시간\n",
        "  sleep(0.01)\n",
        "\n",
        "#nnews naver\n",
        "  if title == 'n.news.naver.com':\n",
        "    request_content = requests.get(content_url)\n",
        "    document_content = BeautifulSoup(request_content.content, 'html.parser')\n",
        "\n",
        "    # 기사 제목 가져옴\n",
        "    tag_headline = document_content.find_all('h2', {'id': 'title_area'})\n",
        "    # 뉴스 기사 제목 초기화\n",
        "    text_headline = ''\n",
        "    text_headline = text_headline + ArticleParser.clear_headline(str(tag_headline[0].find_all(text=True)))\n",
        "    print(text_headline)\n",
        "    # 공백일 경우 기사 제외 처리\n",
        "    if not text_headline:\n",
        "        continue\n",
        "\n",
        "    # 기사 본문 가져옴\n",
        "    tag_content = document_content.find_all('div', {'id': 'contents'})\n",
        "    # 뉴스 기사 본문 초기화\n",
        "    text_sentence = ''\n",
        "    text_sentence = text_sentence + ArticleParser.clear_content(str(tag_content[0].find_all(text=True)))\n",
        "    print(text_sentence)\n",
        "    # 공백일 경우 기사 제외 처리\n",
        "    if not text_sentence:\n",
        "        continue\n",
        "\n",
        "    # 기사 언론사 가져옴\n",
        "    tag_company = document_content.find_all('meta', {'property': 'og:article:author'})\n",
        "    # 언론사 초기화\n",
        "    text_company = ''\n",
        "    text_company = text_company + str(tag_company[0].get('content'))\n",
        "    print(text_company)\n",
        "    # 공백일 경우 기사 제외 처리\n",
        "    if not text_company:\n",
        "        continue\n",
        "\n",
        "    # data append to list\n",
        "    data = [text_company, text_headline, text_sentence, content_url]\n",
        "    file_list .append(data)\n",
        "\n",
        "#인포멕스\n",
        "  elif title == 'news.einfomax.co.kr':\n",
        "    request_content = requests.get(content_url)\n",
        "    document_content = BeautifulSoup(request_content.content, 'html.parser')\n",
        "\n",
        "    # 기사 제목 가져옴\n",
        "    tag_headline = document_content.find_all('h3', {'class': 'heading'})\n",
        "    # 뉴스 기사 제목 초기화\n",
        "    text_headline = ''\n",
        "    text_headline = text_headline + ArticleParser.clear_headline(str(tag_headline[0].find_all(text=True)))\n",
        "    print(text_headline)\n",
        "    # 공백일 경우 기사 제외 처리\n",
        "    if not text_headline:\n",
        "        continue\n",
        "\n",
        "    # 기사 본문 가져옴\n",
        "    tag_content = document_content.find_all('article', {'id': 'article-view-content-div'})\n",
        "    # 뉴스 기사 본문 초기화\n",
        "    text_sentence = ''\n",
        "    text_sentence = text_sentence + ArticleParser.clear_content(str(tag_content[0].find_all(text=True)))\n",
        "    print(text_sentence)\n",
        "    # 공백일 경우 기사 제외 처리\n",
        "    if not text_sentence:\n",
        "        continue\n",
        "\n",
        "    # 기사 언론사 가져옴\n",
        "    tag_company = document_content.find_all('meta', {'name': 'apple-mobile-web-app-title'})\n",
        "    # 언론사 초기화\n",
        "    text_company = ''\n",
        "    text_company = text_company + str(tag_company[0].get('content'))\n",
        "    print(text_company)\n",
        "    # 공백일 경우 기사 제외 처리\n",
        "    if not text_company:\n",
        "        continue\n",
        "\n",
        "    # data append to list\n",
        "    data = [text_company, text_headline, text_sentence, content_url]\n",
        "    file_list .append(data)\n",
        "\n",
        "  else:\n",
        "    pass"
      ],
      "metadata": {
        "id": "_39D-bqrhtVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHvN4D1oD5iR"
      },
      "source": [
        "from pandas import DataFrame\n",
        "\n",
        "#list to dataframe\n",
        "df = DataFrame (file_list,columns=['text_company','text_headline','text_sentence','content_url'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs2Za5R8rCDb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f5de008-87aa-4f9b-f0ad-c03b2f75a317"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzO_lxIBpKER"
      },
      "source": [
        "import csv\n",
        "df.to_csv('/content/drive/MyDrive/Colab Notebooks/amazon_craw.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DPYC64IOS-nh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}